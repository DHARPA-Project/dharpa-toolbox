[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/DHARPA-Project/topic-modelling-prototype-jupyter.git/HEAD)


## Install virtual environment

*Note*: this is only tested on Linux, let me know if it doesn't work for you

```
git clone https://github.com/DHARPA-Project/architecture-documents.git
cd architecture-documents/workflow-modularity/

conda create -n workflow-modularity python=3.7
conda activate workflow-modularity
conda env update --file environment.yml
```

## Description

This folder contains a short presentation and some examples about a possible approach to modularity in our application.

The workflow example that is used is the first part of the topic-modelling workflow, which takes in a dictionary (key: file-id, value: text-content), tokenizes each text, and then gives the user an option to lowercase everything, and remove stopwords (which the user would also have to provide as inputs).

The 2nd part of the presentation shows how to use that workflow itself as a module in another workflow, and how to connect its input to a module that accepts files/file-uploads.

The overall implementation of everything is very bare-bones, and likely to break at the slightest breeze. The auto-rendered input/output widgets are very basic too, and quite terrible from a user-experience
perspective. This is something that can be improved quite a bit, but I didn't have the time for that yet, and primarily wanted to show the concept of auto-generating UIs in that way. Use your imagination on how to take that further...

### Presentation: ``workflow-modularity.ipynb``

#### Usage:

Since it's a normal Jupyter notebook, you can just view/run it via:

```
jupyter notebook workflow-modularity.ipynb
```

### Notebook: ``example_corpus_processing.ipynb``

*Note*: in some cases (but not always) it seems to take a few seconds to load the UI-elements, not sure why

#### Usage

```
jupyter notebook example_corpus_processing.ipynb
```

Using Voila:

```
voila --template=material example_corpus_processing.ipynb
# or, using the default theme
voila  example_corpus_processing.ipynb
```

#### Description

This renders all the input and output fields of the basic workflow, which takes a dictionary as input. To use it, fill use a json-formatted string like below in the 'text_map' field, and a stopword per line (for example: 'hello') in the 'stopwords' one:

```json
{
  "one.txt": "Hello World!",
  "two.txt": "Hello Dharpa!"
}
```

### ``example_input_file_processing.ipynb``

#### Usage

*Note*: in some cases (but not always) it seems to take a few seconds to load the UI-elements, not sure why

Using Jupyter:

```
jupyter notebook example_input_file_processing.ipynb
```

Using Voila:
```
voila --template=material example_input_file_processing.ipynb
# or, using the default theme
voila example_input_file_processing.ipynb
```

#### Description

This notebook renders the 2nd workflow from the presentation, where the workflow from the example before is used as part of another workflow, which basically replaces the 'text_map' input field with a nicer to use upload button. You can use the ``data/text_corpus_<x>.txt`` files included in this repository as input files here.

### Notebook: ```example_input_file_processing_2.ipynb```

#### Usage

*Note*: in some cases (but not always) it seems to take a few seconds to load the UI-elements, not sure why

Using Jupyter:

```
jupyter notebook example_input_file_processing_2.ipynb
```

Using Voila:

```
voila --template=material example_input_file_processing_2.ipynb
# or, using the default theme
voila example_input_file_processing_2.ipynb
```

#### Description

This one renders the same workflow as the previous one, but using a more specialized set of widgets, which enables some preview of processing results for a single file before kicking off the processing for all of them (which could potentially take a while). This is currently not working 100% correctly, but hopefully it shows how it can be possible to render the exact same workflow, by just improving the frond-end part of our application, without having to worry at all about adapting the workflow itself.
Same as above, the ``data/text_corpus_<x>.txt`` files can be used as input files. Also, the ``data/stopwords.csv`` file can be used as stopwords input.


### Autogenerated REST-API: ``workflow_api.py``

#### Usage

```
uvicorn workflow_api:app
```

Then visit the following url in a browser: http://localhost:8000/docs

#### Description

This finds all pre-created modules (in [the dharap-toolbox library](https://github.com/DHARPA-Project/dharpa-toolbox)) and workflows (in the [``workflows`` folder](https://github.com/DHARPA-Project/architecture-documents/tree/master/workflow-modularity/workflows)), and renders a REST-API with an (POST) endpoint for each of them, incl. describing the data that goes in, and comes out.


### Command-line interface (partially auto-generated): ``dharapa-toolbox``

#### Usage

Display command help:

```
$ dharpa-toolbox --help
Usage: dharpa-toolbox [OPTIONS] COMMAND [ARGS]...

Options:
  ...
  ...
```

Display workflow run help:

```
$ dharpa-toolbox run-module input_files_processing --help
Usage: dharpa-toolbox run-module input_files_processing [OPTIONS]

  Reads one or several (text)-files, tokenzies the content, then processes
  the tokenized content according to the provided settings.

  Currently, lowercasing and the removal of stopwords is supported.

Options:
  --files TEXT                    [required]
  --make-lowercase / --no-make-lowercase
                                  [required]
  --remove-stopwords / --no-remove-stopwords
                                  [required]
  --stopwords TEXT                [required]
  --help                          Show this message and exit.
```

Run a workflow:

```
$ dharpa-toolbox run-module input_files_processing --make-lowercase --remove-stopwords --stopwords hello --files data/text_corpus_1.txt
{'processed_text_corpus': {'text_corpus_1.txt': ['world', '!', 'dharpa', '!']}}
```

#### Description

Similar to the auto-generated REST-API, this generates a commandline application that can take the inputs for a workflow run via a terminal command. This is useful if we are interested in running a workflow in batch mode (for long running ones, or in a script), or in other environments (HPC cluster).
